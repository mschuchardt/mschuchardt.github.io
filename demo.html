<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Lead Conversion Model</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<script src="site_libs/viz-0.3/viz.js"></script>
<link href="site_libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="site_libs/grViz-binding-1.0.1/grViz.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 45px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h2 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h3 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h4 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h5 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h6 {
  padding-top: 50px;
  margin-top: -50px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>



<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = false;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-inverse  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"></a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Matt Schuchardt</a>
</li>
<li>
  <a href="demo.html">Modeling</a>
</li>
<li>
  <a href="data.html">Visualization</a>
</li>
<li>
  <a href="match.html">Matching</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://www.linkedin.com/in/mattschuchardt/">
    <span class="fa fa-linkedin"></span>
     
  </a>
</li>
<li>
  <a href="mailto:matt@schu.in">
    <span class="fa fa-envelope"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Lead Conversion Model</h1>
<h3 class="subtitle">Predicting leads likely to convert</h3>

</div>


<pre class="r"><code>object &lt;- &quot;Lead&quot;

object.label &lt;- paste0(object, &quot;s&quot;)

# selection metric ----
metric &lt;- metric.table %&gt;% 
  filter(.metric == &quot;bal_accuracy&quot;)

#start clock
start.time &lt;- proc.time()
api.date &lt;- today()
api.month &lt;- as.character(month(api.date, 
                                label = T, abbr = F))
api.day &lt;- as.character(day(api.date))
api.time &lt;- format(now(), &quot;%H:%M %p %Z&quot;)
api.label &lt;- paste(api.month, api.day, &quot;at&quot;, api.time)

age.limit &lt;- api.date %m-% years(2)

# soql api call ---------
obj.soql &lt;- paste0(
           &quot;SELECT Id,
            Title,
            Email,
            LeadSource,
            Product_Service_Interest__c,
            pi__campaign__c,
            pi__score__c, 
            pi__first_touch_url__c,
            pi__first_activity__c,
            pi__last_activity__c,
            CreatedDate,
            LastModifiedDate,
            LastActivityDate,
            Status,
            IsConverted,
            ConvertedDate FROM Lead WHERE
            Status != &#39;Existing Opportunity&#39;&quot;
            ) %&gt;% 
  str_remove_all(., &quot;\\\n&quot;) %&gt;% 
  str_squish(.)
 
# excute api call
obj &lt;- sf_query(obj.soql, object_name = object, api_type=&quot;Bulk 1.0&quot;) 

# readr import errors
prbs &lt;- attributes(obj)$problems

# fix errors
obj.conv &lt;- obj[prbs$row, &quot;Id&quot;] %&gt;% 
  bind_cols(prbs %&gt;% 
              select(ConvertedDate = actual)) %&gt;% 
  mutate(ConvertedDate = as_date(ConvertedDate)) %&gt;% 
  as_tibble()
  
obj &lt;- obj %&gt;%
  select(-ConvertedDate) %&gt;% 
  left_join(obj.conv) %&gt;% 
  mutate_if(is.POSIXct, as_date) %&gt;% 
  mutate(Closed = !is.na(ConvertedDate) |
           IsConverted == T |
           Status %in% c(&quot;Qualified&quot;,
                         &quot;Unqualified&quot;),
          Closed = ifelse(Status == &quot;Open&quot;, F, Closed),
          LastActivity =
           case_when(!is.na(pi__last_activity__c) &amp;
                       !is.na(LastActivityDate) &amp;
                       pi__last_activity__c &gt; 
                       LastActivityDate ~  as_date(pi__last_activity__c),
                     !is.na(pi__last_activity__c) &amp;
                       !is.na(LastActivityDate) &amp;
                       pi__last_activity__c &lt; 
                       LastActivityDate ~ as_date(LastActivityDate),
                     !is.na(pi__last_activity__c) &amp;
                       is.na(LastActivityDate) ~
                       as_date(pi__last_activity__c),
                     is.na(pi__last_activity__c) &amp;
                       !is.na(LastActivityDate) ~
                       as_date(LastActivityDate),
                     is.na(pi__last_activity__c) &amp;
                       is.na(LastActivityDate) ~
                       as_date(LastModifiedDate),
                     TRUE ~ as_date(LastActivityDate))) %&gt;% 
  filter(LastActivity &gt;= as_date(age.limit))

# feature engineering ----
# create labels
# create day variables
# other indicator variables
# extract email suffix
# aggregate levels for variables with 30+ levels
# via string detection
objs &lt;- obj %&gt;% 
  separate(Email,
           c(&quot;drop&quot;, &quot;domain&quot;), 
           sep = &quot;@&quot;,
           remove = T) %&gt;% 
  select(-drop) %&gt;% 
  mutate(label = case_when(Closed == T &amp; 
                             IsConverted == T |
                             Status == &quot;Qualified&quot; ~
                             &quot;Yes&quot;,
                             Closed == T &amp;
                             IsConverted == F |
                             Status == &quot;Unqualified&quot; ~
                             &quot;No&quot;),
         label = factor(label, levels = c(&quot;Yes&quot;, &quot;No&quot;)),
         PublicDomain = ifelse(domain %in% public.domains, 
                               &quot;Yes&quot;, &quot;No&quot;),
         PublicDomain = factor(PublicDomain),
         KnownURL = 
           ifelse(is.na(pi__first_touch_url__c), &quot;No&quot;,
                  &quot;Yes&quot;),
         KnownURL = factor(KnownURL),
         EmailSuffix =
           as.character(str_extract_all(domain, 
                                        &quot;[.][A-z]{2,6}$&quot;)),
          EmailSuffix =str_remove_all(EmailSuffix, &quot;[.]&quot;),
         Age = 
           ifelse(!is.na(ConvertedDate),
                   as.numeric(ConvertedDate -
                                CreatedDate),
                as.numeric(lubridate::as_date(Sys.Date()) -
                      CreatedDate)),
         Age = ifelse(Age == 0, 1, Age),
         ActivityDays = 
           ifelse(!is.na(ConvertedDate),
                    as.numeric(ConvertedDate - 
                                 LastActivity),
                as.numeric(lubridate::as_date(Sys.Date()) -
                              LastActivity))) %&gt;% 
  select(-domain)

# training/testing records ----
known.objs &lt;- objs %&gt;% 
  filter(!is.na(label))%&gt;% 
  select(Id,
         Age,
         ActivityDays,
         pi__score__c,
         pi__campaign__c,
         EmailSuffix,
         Product_Service_Interest__c,
         LeadSource,
         Title,
         PublicDomain,
         KnownURL,
         label)

# initial split ----
# create holdout set
initial.split &lt;- rsample::initial_split(known.objs, 
                                        prop = 3/4,
                                        strata = &quot;label&quot;)

# cut training records
training &lt;- training(initial.split)

limit.date &lt;- paste(as.character(month(age.limit,
                                       label = T, 
                                       abbr = F)),
                    year(age.limit))</code></pre>
<p>The workflow proposed by <a href="http://www.feat.engineering/">Kuhn and Johnson (2019)</a> was implemented using <a href="https://github.com/tidymodels">tidymodel</a> tools.</p>
<p>Model development is inside a two-layer nested resampling scheme. This leads to a more robust model, enhanced confidence in development decisions and increased computation time. To generate a single scoring run over 1,000 models will be fit to more than 100 distinct datasets.</p>
<p>Using resampling to create multiple distinct training sets prevents <strong>model overfitting</strong>. To reduce the risk of <strong>overfitting predictors</strong> features are engineered during each resample. Any specious relationship between the predictors and outcome is unlikely throughout the resamples. Measuring performance across a series of distinct resamples reduces the likelihood that <strong>performance estimates will be overly optimistic</strong>.</p>
<p>The performance measure used to select each model parameter was <strong>Balanced Accuracy</strong>, computed as average of sensitivity and specificity. Bayesian model analysis, as outlined by <a href="http://people.idsia.ch/~marco/papers/2017jmlr-tests.pdf">Benavoli et al (2017)</a>, was used to validate each selection.</p>
<pre class="r"><code># variable significance -----------
# fit an intercept only logistic model
# use add1 to fit single variable models for other predictors
# select predictors that significantly improve deviance
var_sig &lt;- function(df, vrs){
  .scp &lt;- as.formula(paste(&quot;~&quot;, paste(vrs, collapse = &quot;+&quot;)))
  df &lt;- mutate(df, label = ifelse(label == &quot;Yes&quot;, 1, 0))
  mdl &lt;- glm(label ~ 1,
             family = binomial(link = &quot;logit&quot;),
             data = df)
  add &lt;- add1(mdl,
              scope = .scp) %&gt;% 
    rownames_to_column(&quot;variable&quot;) %&gt;% 
    mutate(dev.imp = mdl$deviance - Deviance,
           ptst = 1-pchisq(dev.imp, 1)) %&gt;% 
    filter(variable %in% vrs) %&gt;% 
    filter(ptst &lt; 0.01) 
  out &lt;- add$variable
  rm(mdl, df, .scp, add)
  gc()
  return(out)
}

# variable relevance ----
# compare variable importance to importance achievable at random
# via random forest
# select variables deemed relevant
var_rel &lt;- function(df, form){
  bor &lt;- Boruta(form, df) %&gt;% 
    TentativeRoughFix()
  out &lt;- getSelectedAttributes(bor)
  return(out)
}

# variable importance ----
# use caret to calculate variable importance
# select important variables
var_imp &lt;- function(df, .form){
  mdl &lt;- caret::train(.form, data = df, method = &quot;glmnet&quot;)
  imp &lt;- caret::varImp(mdl)
  imp &lt;- imp$importance %&gt;% 
    rownames_to_column(&quot;vrs&quot;) %&gt;% 
    filter(Overall &gt; 0)
  out &lt;- imp$vrs
  return(out)
}

# combined variable importance ----
final_imp &lt;- function(df, vrs){
  form &lt;- as_formula(vrs)
  .scp &lt;- as.formula(paste(&quot;~&quot;, paste(vrs, collapse = &quot;+&quot;)))
  var.imp &lt;- caret::train(form, data = df, method = &quot;glmnet&quot;) %&gt;% 
    caret::varImp()
  var.imp &lt;- var.imp$importance %&gt;% 
    rownames_to_column(&quot;variable&quot;) %&gt;% 
    dplyr::select(variable, value = Overall)
  var.rel &lt;- Boruta(form, df) %&gt;% 
    TentativeRoughFix()
  var.rel &lt;- var.rel$ImpHistory %&gt;% 
    as_tibble() %&gt;% 
    gather(variable, value) %&gt;% 
    mutate(value = ifelse(!is.finite(value), 0, value)) %&gt;% 
    dplyr::group_by(variable) %&gt;% 
    dplyr::summarize(value = mean(value)) %&gt;% 
    ungroup() %&gt;% 
    dplyr::filter(is.finite(value)) %&gt;% 
    dplyr::filter(variable %in% vrs)
  dat &lt;- mutate(df, label = ifelse(label == &quot;Yes&quot;, 1, 0))
  mdl &lt;- glm(label ~ 1,
             family = binomial(link = &quot;logit&quot;),
             data = dat)
  add &lt;- add1(mdl,
              scope = .scp) %&gt;% 
    rownames_to_column(&quot;variable&quot;) %&gt;% 
    dplyr::mutate(dev.imp = mdl$deviance - Deviance,
                  ptst = 1-pchisq(dev.imp, 1)) %&gt;% 
    dplyr::filter(variable %in% vrs) %&gt;% 
    dplyr::select(variable, value = dev.imp)
  out &lt;- var.rel %&gt;% 
    dplyr::mutate(selector = &quot;Relevance&quot;) %&gt;% 
    bind_rows(var.imp %&gt;% 
                dplyr::mutate(selector = &quot;Importance&quot;)) %&gt;% 
    bind_rows(add %&gt;% 
                dplyr::mutate(selector = &quot;Significance&quot;)) %&gt;% 
    group_by(selector) %&gt;% 
    dplyr::mutate(value = value / max(value)) %&gt;% 
    ungroup()
  return(out)
}

# level aggregation ----
# most freq token
token_freq &lt;- function(x){
tkns &lt;- tokenizers::tokenize_word_stems(x,
                                        stopwords = 
                                          c(&quot;of&quot;, &quot;the&quot;, &quot;and&quot;))
tkn.freq &lt;- table(unlist(tkns))
for(i in seq_along(tkns)){
  tkns[[i]] &lt;- names(which.max(tkn.freq[tkns[[i]]]))[[1]]
  tkns[[i]] &lt;- ifelse(tkns[[i]] == &quot;NA&quot;, NA, tkns[[i]])
  tkns
}
return(unlist(tkns))
}

# bayes ttest ----
# workhorse ----
correlatedBayesianTtest &lt;- function(diff_a_b,rho,rope_min,rope_max){
  if (rope_max &lt; rope_min){
    stop(&quot;rope_max should be larger than rope_min&quot;)
  }
  
  delta &lt;- mean(diff_a_b)
  n &lt;- length(diff_a_b)
  df &lt;- n-1
  stdX &lt;- sd(diff_a_b)
  sp &lt;- sd(diff_a_b)*sqrt(1/n + rho/(1-rho))
  p.left &lt;- pt((rope_min - delta)/sp, df)
  p.rope &lt;- pt((rope_max - delta)/sp, df)-p.left
  results &lt;- list(&#39;left&#39;=p.left,&#39;rope&#39;=p.rope,
                  &#39;right&#39;=1-p.left-p.rope)
  return (results)
}

# prep ------
# by creating ab list
pair_cross &lt;- function(df, .grp){
  crss &lt;- as_tibble(list(&quot;a&quot; = df[[.grp]],
                         &quot;b&quot; = c(df[[.grp]][2:nrow(df)], NA))) %&gt;% 
    filter(!is.na(b))
  return(crss)
}

# compute differences ----
# and apply bayesian ttest  
corr_bayes_test &lt;- function(x, 
                            df, 
                            flds, 
                            best,
                            .ropemin = -0.01,
                            .ropemax = 0.01){
  out &lt;- correlatedBayesianTtest(df[[best]] - df[[x]],
                                 rho = flds,
                                 rope_min = .ropemin,
                                 rope_max = .ropemax)
  return(out)
}

# execute ----
# for entire ab list and label results
bayes_ttest &lt;- function(df.rank, .grp, df.summary, .flds){
 crs &lt;- pair_cross(df.rank, .grp)
 a.b &lt;- vector(&quot;list&quot;, nrow(crs))
 for(i in 1:nrow(crs)){
   a.b[[i]] &lt;- corr_bayes_test(x = crs[[i,1]],
                               df = df.summary,
                               flds = 1/.flds,
                               best = crs[[i,2]])
 names(a.b[[i]]) &lt;- c(paste(crs[[i,1]], 
                                  &quot;&gt;&quot;,
                                  crs[[i,2]]),
                            paste(crs[[i,1]],
                                  &quot;=&quot;,
                                  crs[[i,2]]),
                            paste(crs[[i,1]], 
                                  &quot;&lt;&quot;,
                                  crs[[i,2]]))
  a.b[[i]] &lt;- a.b[[i]] %&gt;% 
    as_tibble()
  a.b[[i]]
 }
 return(a.b)
}

ab_test &lt;- function(df.rank, df.test, .met){
  if(.met == &quot;mn_log_loss&quot;){
    out &lt;- bayes_ttest(df.rank, &quot;model&quot;, df.test, 
                        .flds = length(unique(df.test$id))) %&gt;% 
  map(., ~gather(., key, prob)) %&gt;% 
  bind_rows(.id = &quot;pair&quot;) %&gt;% 
  mutate(equal.better = str_detect(key, &quot;&lt;&quot;),
         equal.worse = str_detect(key, &quot;&gt;&quot;)) %&gt;%
      group_by(pair) %&gt;% 
      mutate(rw = row_number()) %&gt;% 
      arrange(desc(rw), .by_group = T) %&gt;% 
  group_by(pair, equal.better) %&gt;% 
  mutate(combined = sum(prob)) %&gt;% 
    group_by(pair, equal.worse) %&gt;% 
  mutate(less = sum(prob)) %&gt;% 
  ungroup()%&gt;% 
  separate(key, c(&quot;better&quot;, &quot;than&quot;), 
           sep = &quot; &gt; | = | &lt;&quot;, remove = F) %&gt;% 
  mutate(better = str_trim(better, &quot;both&quot;),
         than = str_trim(than, &quot;both&quot;))
      return(out)
  }else{
    out &lt;- bayes_ttest(df.rank, &quot;model&quot;, df.test, 
                        .flds = length(unique(df.test$id))) %&gt;% 
  map(., ~gather(., key, prob)) %&gt;% 
  bind_rows(.id = &quot;pair&quot;) %&gt;% 
  mutate(equal.better = str_detect(key, &quot;=|&gt;&quot;),
         equal.worse = str_detect(key, &quot;=|&lt;&quot;)) %&gt;% 
  group_by(pair, equal.better) %&gt;% 
  mutate(combined = sum(prob)) %&gt;% 
    group_by(pair, equal.worse) %&gt;% 
  mutate(less = sum(prob)) %&gt;% 
  ungroup()%&gt;% 
  separate(key, c(&quot;better&quot;, &quot;than&quot;), 
           sep = &quot; &gt; | = | &lt;&quot;, remove = F) %&gt;% 
  mutate(better = str_trim(better, &quot;both&quot;),
         than = str_trim(than, &quot;both&quot;))
      return(out)
  }
}

# helper functions ----
# quiet fits ----
# don&#39;t print model details to console
q_f &lt;- quietly(fit)

# intersect 3 vectors
triple_u &lt;- function(x, y, z){
  p1 &lt;- as_tibble(list(&quot;vrs&quot; = c(x,y,z))) %&gt;% 
    unique()
  out &lt;- unique(p1$vrs)
  return(out)
}

# union 3 vectors
triple_inter &lt;- function(x, y, z){
  p1 &lt;- as_tibble(list(&quot;vrs&quot; = c(x,y,z))) %&gt;% 
    mutate(n = 1) %&gt;% 
    group_by(vrs) %&gt;% 
    filter(n() == 3)
  out &lt;- unique(p1$vrs)
  return(out)
}

# prep folds ----
fold_recipe &lt;- function(df, .rec){
  out &lt;- df %&gt;%
    mutate(analysis = map(splits, as.data.frame, data = &quot;analysis&quot;),
           assessment =  map(splits, as.data.frame, data = &quot;assessment&quot;),
           prep = map(analysis, df_prep, rec = .rec),
           analysis = map2(prep, analysis, bake),
           assessment = map2(prep, assessment, bake),
           form = map(prep, formula))
  return(out)
}

# parsnip formula fitter ----
# reorder arguments to use on list columns
prs_form &lt;- function(df, .form, prs){
  out &lt;- q_f(prs, .form, df)[[&quot;result&quot;]]
  return(out)
}

# recipe prepper ----
# reorder arguments to use on list columns
df_prep &lt;- function(df, rec){
  out &lt;- prep(rec, df, strings_as_factors = F)
  return(out)
}

# predictor ----
# reorder arguments to use on list columns
df_pred &lt;- function(df, .fit, .type){
  out &lt;- predict(.fit, new_data = df, type = .type)
}

# formula creator ----
# create formula from vector of variables
as_formula &lt;- function(.vrs, y.var = &quot;label&quot;){
  out &lt;- as.formula(paste(y.var, &quot;~&quot;,
                          paste(.vrs, collapse = &quot;+&quot;)))
  return(out)
}

# pads for unnesting
df_pad &lt;- function(df, x){
  out &lt;- df[x,]
}

# plot lines
plot.grid &lt;- as_tibble(list(&quot;grid&quot; = seq(1.5, 4.5, 1)))

zero.rw &lt;- plot.grid[0,]

prose_title &lt;- function(a.b, .type, .same = zero.rw){
  if(a.b[1,]$prob &gt; 0.7){
    paste0(&quot;The probability &quot;,
           a.b[1, ]$better,
           &quot; is the best &quot;,
           .type,
           &quot; is &quot;,
           round(a.b[1, ]$prob,2)*100,
           &quot;%&quot;)
  }else{
    if(nrow(.same) == 1){
      paste0(a.b[1, ]$than,
             &quot; selects &quot;,
             round((.same[1,]$than.vrs/.same[1,]$better.vrs) *100),
             &quot;% fewer features than &quot;,
             a.b[1, ]$better,
           &quot;\nand the probability it performs as well or better is &quot;,
             round(.same$less[1],2)*100,
             &quot;%&quot;)
    }else{
      paste0(&quot;The probability &quot;,
             a.b[1, ]$better,
             &quot; performs as well or better than &quot;,
             a.b[1, ]$than,
             &quot; is &quot;,
             round(a.b$combined[1],2)*100,
             &quot;%&quot;)
    }
  }
}

# plot folds ----
plot_folds &lt;- function(df, .tlt, .sbtlt){
  p &lt;- df %&gt;% 
    filter(.metric == metric$.metric) %&gt;% 
    ggplot(aes(id2, 
               .estimate, 
               group = fct_rev(fct_reorder(model,
                                           .estimate,
                                           .fun = mean)),
               color = fct_rev(fct_reorder(model,
                                           .estimate,
                                           .fun = mean))))+
    stat_summary(fun.y = &quot;max&quot;,
                geom = &quot;point&quot;,
                shape = 95,
                size = 2.5,
                stroke = 4,
                position = position_dodge(width = 0.9))+
    stat_summary(fun.y = &quot;min&quot;,
                geom = &quot;point&quot;,
                shape = 95,
                size = 4,
                stroke = 4,
                position = position_dodge(width = 0.9))+
    stat_summary(fun.data = &quot;median_hilow&quot;,
                geom = &quot;linerange&quot;,
                size = .75,
                alpha = 1/2,
                linetype = 3,
                position = position_dodge(width = 0.9))+
    geom_vline(data = plot.grid, aes(xintercept = grid),
               color = &quot;grey50&quot;,
               size = 0.25)+
    stat_summary(fun.data = &quot;mean_cl_boot&quot;,
                 geom = &quot;line&quot;,
                 position = position_dodge(width = 0.9))+
    stat_summary(fun.data = &quot;mean_cl_boot&quot;,
                 geom = &quot;point&quot;,
                 position = position_dodge(width = 0.9))+
    guides(color = guide_legend(title = NULL))+
    scale_color_brewer(palette = &quot;Set2&quot;)+
    labs(title = paste(.tlt),
         subtitle = paste(.sbtlt),
         x = NULL,
         y = paste(metric$metric))+
    theme(panel.background =
           element_rect(fill = NA,
                        colour = &quot;grey50&quot;),
          legend.key = element_rect(fill = &quot;white&quot;),
          legend.key.size = unit(2, &quot;lines&quot;))
  print(p)
}

# compute summary metrics
tune_summary &lt;- function(df){
  out &lt;- df %&gt;% 
  conf_mat(truth = label, .pred_class) %&gt;% 
  mutate(summary = map(conf_mat, summary)) %&gt;% 
  ungroup() %&gt;% 
  select(id, id2, model, summary) %&gt;% 
  unnest() %&gt;% 
  bind_rows(folds.pred %&gt;% 
              mn_log_loss(truth = label, .pred_Yes)) %&gt;% 
  mutate(comp = metric_benchmark(.metric, .estimate))
  return(out)
}

# rank based on selected metric
tune_rank &lt;- function(df, .met){
  out &lt;- df %&gt;% 
  filter(.metric == .met) %&gt;% 
  group_by(model, .metric) %&gt;% 
  summarize(.estimate = mean(.estimate)) %&gt;% 
  group_by(.metric) %&gt;% 
  mutate(rank = case_when(.metric ==
                            &quot;mn_log_loss&quot; ~ dense_rank(.estimate),
                          TRUE ~ dense_rank(desc(.estimate)))) %&gt;% 
  ungroup() %&gt;% 
  arrange(rank)
  return(out)
}

# prepare for bayes ttest
tune_test &lt;- function(df, .met){
  out &lt;- df %&gt;% 
  filter(.metric == .met) %&gt;% 
  select(id, id2, model, .metric, .estimate) %&gt;% 
  spread(model, .estimate, fill = 0) %&gt;% 
  arrange(.metric, id, id2)
  return(out)
}</code></pre>
<div id="nested-resamples" class="section level2">
<h2>Nested Resamples</h2>
<pre class="r"><code># resamples ----
# external ----
# create external folds
folds.external &lt;- rsample::vfold_cv(training,
                                    v = 5,
                                    repeats = 1,
                                    strata = &quot;label&quot;) 

# extract df of external folds
analysis.external &lt;- folds.external$splits %&gt;% 
  map(., as.data.frame, data = &quot;analysis&quot;)

assess.external &lt;- folds.external$splits %&gt;% 
  map(., as.data.frame, data = &quot;assessment&quot;)

# internal ----
# create internal folds
folds.internal.mdl &lt;- rsample::vfold_cv(analysis.external[[1]],
                                        v = 5,
                                        repeats = 10,
                                        strata = &quot;label&quot;) 

folds.internal.fs &lt;- rsample::vfold_cv(analysis.external[[2]],
                                        v = 5,
                                        repeats = 10,
                                        strata = &quot;label&quot;) 

folds.internal.tn &lt;- rsample::vfold_cv(analysis.external[[3]],
                                        v = 5,
                                        repeats = 1,
                                        strata = &quot;label&quot;) 

# records per fold
fold.size &lt;- tibble::tribble(
  ~&quot;Set&quot;, ~&quot;Size&quot;,
  &quot;Known&quot;, nrow(known.objs),
  &quot;Train&quot;, nrow(training),
  &quot;Holdout&quot;, nrow(rsample::testing(initial.split)),
  &quot;Analysis&quot;, map(folds.internal.mdl$splits,
                  as.data.frame,
                  data = &quot;analysis&quot;) %&gt;% 
    map_dbl(., nrow) %&gt;% 
    mean() %&gt;% 
    round(),
  &quot;Assessment&quot;, map(folds.internal.mdl$splits,
                  as.data.frame,
                  data = &quot;assessment&quot;) %&gt;% 
    map_dbl(., nrow) %&gt;% 
    mean() %&gt;% 
    round()
  ) %&gt;% 
  mutate(Ratio = (Size / max(Size)) * 2.25)</code></pre>
<p><strong>Initial Split</strong> The 416 records with known responses were split into a 313 record training set and a 103 record holdout set.</p>
<p><strong>External Resamples</strong> The training records were resampled to create 5 cross-validation folds each containing<br />
250.4 analysis records and 62.6 assessment records.</p>
<p><strong>Internal Resamples</strong> The analysis records from each external fold were repeatedly resampled, creating 10 repeats of 5 fold cross-validation sets, each containing 200 analysis and 50 assessment records.</p>
<p><strong>Final Model</strong> The selected parameters were used to fit the final model to all 313 training records and then validated against the 103 record holdout set.</p>
<p>A simplified diagram of the resampling scheme is below.</p>
<pre class="r"><code># draw resampling diagram  
DiagrammeR::grViz(&quot;digraph nested_resamples 
{

  # a &#39;graph&#39; statement
  graph [overlap = true, fontsize = 10]

  # several &#39;node&#39; statements
  node [shape = pentagon,
        fontname = Helvetica,
        label = &#39;Known Records&#39;,
        fixedsize = true,
        height = 2.25,
        width = 2.25]
  Known 

  node [shape = square,
        color = DarkBlue,
        fill = SeaShell,
        label = &#39;Train&#39;,
        fixedsize = true,
        height = 1.7,
        width = 1.7]
  Train 

  node [shape = house,
        fontcolor = DarkSlateGray,
        fill = LightCyan,
        label = &#39;Test&#39;,
        fizedsize = true,
        height = 0.55,
        width = 0.55]
  Test 
  
  node [shape = plaintext,
        fontcolor = DarkBlue,
        label = &#39;External&#39;]
  External1
  
  node [shape = plaintext,
        fontcolor = DarkBlue,
        label = &#39;Resample&#39;]
  ExternalA

  node [shape = rectangle,
        fontcolor = SteelBlue,
        color = SteelBlue,
        label = &#39;Classifier\nSelection&#39;,
        fixedsize = true,
        height = 0.6,
        width = 1.1]
  Fold1

  node [shape = rectangle,
        fontcolor = SlateBlue,
        color = SlateBlue,
        label = &#39;Feature\nSelection&#39;,
        fixedsize = true,
        height = 0.6,
        width = 1.1]
  FoldA 

  node [shape = plaintext,
        fontcolor = SteelBlue,
        color = SteelBlue,
        label = &#39;Internal Resamples&#39;]
  Internal1

  node [shape = plaintext,
        fontcolor = SlateBlue,
        color = SlateBlue,
        label = &#39;Internal Resamples&#39;]
  InternalA 

  node [shape = square,
        fontcolor = SteelBlue,
        color = SteelBlue,
        fill = SeaShell,
        label = &#39;Analysis&#39;,
        fixedsize = true,
        height = 1,
        width = 1]
  train1; train2
  
  node [shape = square,
        fontcolor = SlateBlue,
        color = SlateBlue,
        fill = SeaShell,
        label = &#39;Analysis&#39;,
        fixedsize = true,
        height = 1,
        width = 1]
  trainA; trainB

  node [shape = house,
        fontcolor = DarkSlateGray,
        color = SteelBlue,
        fill = LightCyan,
        label = &#39;Test&#39;,
        fixedsize = true,
        height = 0.4,
        width = 0.4]
  test1; test2

  node [shape = house,
        fontcolor = DarkSlateGray,
        color = SlateBlue,
        fill = LightCyan,
        label = &#39;Test&#39;,
        fixedsize = true,
        height = 0.4,
        width = 0.4]
  testA; testB

  subgraph {
    rank = same; Test; Train;
  }

  subgraph {
    rank = same; External1; ExternalA;
  }

  subgraph {
    rank = same; Fold1; FoldA;
  }

  subgraph {
    rank = same; Internal1; InternalA;
  }
  
  subgraph {
    rank = same; train1; test1; train2; test2; 
                 trainA; testA, trainB; testB;
  }

  # several &#39;edge&#39; statements
  Known-&gt;Test [arrowhead = none]
  Known-&gt;Train
  Train-&gt;Test[arrowhead = none]
  Train-&gt;{External1 ExternalA}[arrowhead = none]
  External1-&gt;Fold1
  ExternalA-&gt;FoldA
  Fold1-&gt;Internal1 [color = SteelBlue, arrowhead = none]
  Internal1-&gt;{train1 train2} [color = SteelBlue]
  Internal1-&gt;{test1 test2}[color = SteelBlue,arrowhead = none]
  FoldA-&gt;InternalA [color = SlateBlue, arrowhead = none]
  InternalA-&gt;{trainA trainB} [color = SlateBlue]
  InternalA-&gt;{testA testB}[color = SlateBlue,arrowhead = none]
  train1-&gt;test1 [color = SteelBlue, arrowhead = none]
  train2-&gt;test2 [color = SteelBlue, arrowhead = none]
  trainA-&gt;testA [color = SlateBlue, arrowhead = none]
  trainB-&gt;testB [color = SlateBlue, arrowhead = none]
}
&quot;)</code></pre>
<div id="htmlwidget-8e0f3eb807708c99bdb3" style="width:576px;height:384px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-8e0f3eb807708c99bdb3">{"x":{"diagram":"digraph nested_resamples \n{\n\n  # a \"graph\" statement\n  graph [overlap = true, fontsize = 10]\n\n  # several \"node\" statements\n  node [shape = pentagon,\n        fontname = Helvetica,\n        label = \"Known Records\",\n        fixedsize = true,\n        height = 2.25,\n        width = 2.25]\n  Known \n\n  node [shape = square,\n        color = DarkBlue,\n        fill = SeaShell,\n        label = \"Train\",\n        fixedsize = true,\n        height = 1.7,\n        width = 1.7]\n  Train \n\n  node [shape = house,\n        fontcolor = DarkSlateGray,\n        fill = LightCyan,\n        label = \"Test\",\n        fizedsize = true,\n        height = 0.55,\n        width = 0.55]\n  Test \n  \n  node [shape = plaintext,\n        fontcolor = DarkBlue,\n        label = \"External\"]\n  External1\n  \n  node [shape = plaintext,\n        fontcolor = DarkBlue,\n        label = \"Resample\"]\n  ExternalA\n\n  node [shape = rectangle,\n        fontcolor = SteelBlue,\n        color = SteelBlue,\n        label = \"Classifier\nSelection\",\n        fixedsize = true,\n        height = 0.6,\n        width = 1.1]\n  Fold1\n\n  node [shape = rectangle,\n        fontcolor = SlateBlue,\n        color = SlateBlue,\n        label = \"Feature\nSelection\",\n        fixedsize = true,\n        height = 0.6,\n        width = 1.1]\n  FoldA \n\n  node [shape = plaintext,\n        fontcolor = SteelBlue,\n        color = SteelBlue,\n        label = \"Internal Resamples\"]\n  Internal1\n\n  node [shape = plaintext,\n        fontcolor = SlateBlue,\n        color = SlateBlue,\n        label = \"Internal Resamples\"]\n  InternalA \n\n  node [shape = square,\n        fontcolor = SteelBlue,\n        color = SteelBlue,\n        fill = SeaShell,\n        label = \"Analysis\",\n        fixedsize = true,\n        height = 1,\n        width = 1]\n  train1; train2\n  \n  node [shape = square,\n        fontcolor = SlateBlue,\n        color = SlateBlue,\n        fill = SeaShell,\n        label = \"Analysis\",\n        fixedsize = true,\n        height = 1,\n        width = 1]\n  trainA; trainB\n\n  node [shape = house,\n        fontcolor = DarkSlateGray,\n        color = SteelBlue,\n        fill = LightCyan,\n        label = \"Test\",\n        fixedsize = true,\n        height = 0.4,\n        width = 0.4]\n  test1; test2\n\n  node [shape = house,\n        fontcolor = DarkSlateGray,\n        color = SlateBlue,\n        fill = LightCyan,\n        label = \"Test\",\n        fixedsize = true,\n        height = 0.4,\n        width = 0.4]\n  testA; testB\n\n  subgraph {\n    rank = same; Test; Train;\n  }\n\n  subgraph {\n    rank = same; External1; ExternalA;\n  }\n\n  subgraph {\n    rank = same; Fold1; FoldA;\n  }\n\n  subgraph {\n    rank = same; Internal1; InternalA;\n  }\n  \n  subgraph {\n    rank = same; train1; test1; train2; test2; \n                 trainA; testA, trainB; testB;\n  }\n\n  # several \"edge\" statements\n  Known->Test [arrowhead = none]\n  Known->Train\n  Train->Test[arrowhead = none]\n  Train->{External1 ExternalA}[arrowhead = none]\n  External1->Fold1\n  ExternalA->FoldA\n  Fold1->Internal1 [color = SteelBlue, arrowhead = none]\n  Internal1->{train1 train2} [color = SteelBlue]\n  Internal1->{test1 test2}[color = SteelBlue,arrowhead = none]\n  FoldA->InternalA [color = SlateBlue, arrowhead = none]\n  InternalA->{trainA trainB} [color = SlateBlue]\n  InternalA->{testA testB}[color = SlateBlue,arrowhead = none]\n  train1->test1 [color = SteelBlue, arrowhead = none]\n  train2->test2 [color = SteelBlue, arrowhead = none]\n  trainA->testA [color = SlateBlue, arrowhead = none]\n  trainB->testB [color = SlateBlue, arrowhead = none]\n}\n","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
</div>
<div id="parameters" class="section level1 tabset tabset-pills">
<h1>Parameters</h1>
<pre class="r"><code>vrs.numeric &lt;- colnames(training)[sapply(training, is.numeric)]

vrs.levels &lt;- setdiff(colnames(training)[sapply(training, is.character)],
                      c(&quot;Id&quot;, &quot;label&quot;))

# recipe ----------
# impute
# reduce levels
# create polynomials, interactions &amp; dummy variables
rec.all &lt;- recipe(label ~ ., data = training) %&gt;% 
  update_role(Id, new_role = &quot;id.var&quot;) %&gt;% 
  step_modeimpute(all_nominal(), 
                  -all_outcomes(),
                  -Id) %&gt;% 
  step_mutate(Title = token_freq(Title)) %&gt;% 
  step_mutate(Product_Service_Interest__c =
                token_freq(Product_Service_Interest__c)) %&gt;% 
  step_mutate(LeadSource = token_freq(LeadSource)) %&gt;% 
  step_other(all_nominal(),
             threshold = 0.05,
             other = &quot;Other&quot;,
             -all_outcomes(), 
             -Id) %&gt;% 
  step_knnimpute(all_numeric(), 
                 neighbors = 3, 
                 -all_outcomes()) %&gt;% 
  step_dummy(all_nominal(), 
             -all_outcomes(), -Id) %&gt;% 
  step_interact(terms = ~ 
                  one_of(vrs.numeric):contains(&quot;_&quot;)) %&gt;% 
  step_poly(one_of(vrs.numeric),
            options = list(degree = 4),
            -all_outcomes(),
            -Id) %&gt;% 
  step_lincomb(all_predictors(),
               -all_outcomes(), -Id) %&gt;% 
  step_zv(all_predictors(),
          -all_outcomes(), -Id) %&gt;% 
  step_nzv(all_predictors(),
           -all_outcomes(), -Id) %&gt;% 
  step_corr(all_predictors(), 
            threshold = 0.7,
            method = &quot;pearson&quot;,
            -all_outcomes(), -Id) %&gt;% 
  step_center(all_predictors(),
              -all_outcomes(), -Id) %&gt;% 
  step_scale(all_predictors(),
             -all_outcomes(), -Id) 

# parsnip model setup ------------
# rf specification
prs.rf &lt;- rand_forest(mode = &quot;classification&quot;,
                      mtry = .preds(),
                      trees = 2000) %&gt;% 
  set_engine(&quot;ranger&quot;,
             importance = &quot;impurity&quot;,
             probability = TRUE)

# svm specification
prs.svm &lt;- svm_rbf(mode = &quot;classification&quot;) %&gt;% 
  set_engine(&quot;kernlab&quot;,
             kernel = &quot;rbfdot&quot;,
             prob.model = T)

# xgb specification
prs.xgb &lt;- boost_tree(mode = &quot;classification&quot;) %&gt;% 
  set_engine(&quot;xgboost&quot;,
             prob.model = T)


# list of parsnip model specs
models &lt;- list(&quot;RF&quot; = prs.rf,
               &quot;SVM&quot; = prs.svm,
               &quot;XGB&quot; = prs.xgb)</code></pre>
<div id="features" class="section level2">
<h2>Features</h2>
<p>The following steps were applied individually to each internal resample:</p>
<ol style="list-style-type: decimal">
<li>The most frequent value was used to impute missing categorical variables<br />
</li>
<li>Categorical responses were aggragated based on word stem frequency and rare responses were grouped into an ‘other’ category<br />
</li>
<li>Categorical variables were contrast coded into numeric predictors<br />
</li>
<li>Nearest neighbor was used to impute missing numeric variables<br />
</li>
<li>Two-way interactions were created between dummy categorical predictors and numeric predictors<br />
</li>
<li>Fourth degree polynomial predictors were created from the numeric variables<br />
</li>
<li>Filters were applied to remove predictors: - with no variance<br />
- with sparse or unbalanced responses<br />
- that were highly correlated<br />
- with multicollinearity<br />
</li>
<li>Predictors were centered and scaled</li>
</ol>
</div>
<div id="tuning" class="section level2 tabset">
<h2>Tuning</h2>
<div id="classifier" class="section level3">
<h3>Classifier</h3>
<p>3 classifiers under consideration.</p>
<pre class="r"><code># resample analysis ----
# apply recipe to analysis set of each fold individually
# add columns with analysis and assessment data to split
# prep analysis and assessment sets
folds.internal &lt;- fold_recipe(folds.internal.mdl, .rec = rec.all) %&gt;%
    select(-splits)

# add formula for each fold
# fit each model to analysis set of each fold individually
# generate predictions for assessment set
folds.internal &lt;- folds.internal %&gt;% 
  mutate(RF = map2(analysis, form, prs_form, 
                   prs = models[[&quot;RF&quot;]]),
         RF = map2(RF, assessment, predict, type = &quot;prob&quot;),
         SVM = map2(analysis, form, prs_form, 
                    prs = models[[&quot;SVM&quot;]]),
         SVM = map2(SVM, assessment, predict, type = &quot;prob&quot;),
         XGB = map2(analysis, form, prs_form, 
                    prs = models[[&quot;XGB&quot;]]),
         XGB = map2(XGB, assessment, predict, type = &quot;prob&quot;)) %&gt;% 
  mutate(truth = map(assessment, ~select(., Id, label))) %&gt;% 
  select(id, id2, truth, RF, SVM, XGB) %&gt;% 
  gather(model, preds, -c(id, id2, truth))

# to unnest list columns, df list-elements need equal rows
# get rows to pad by list element
pads &lt;- map_dbl(folds.internal$truth, nrow)
pads &lt;- max(pads) - pads

# pad df
pad.df &lt;- as_tibble(list(&quot;Id&quot; = &quot;0&quot;))

# list of df to pad truth column
pads.id &lt;- map(pads, df_pad, df = pad.df)

# list of df to pad prediction columns
pad.df &lt;- as_tibble(list(&quot;.pred_Yes&quot; = 0))
pads.pd &lt;- map(pads, df_pad, df = pad.df)

# evenup rows
# gather and unnest
# add naive class
# drop padding
# drop models that fail to predict both outcomes
folds.pred &lt;- folds.internal %&gt;% 
   mutate(truth = map2(truth, pads.id, bind_rows),
          preds = map2(preds, pads.pd, bind_rows)) %&gt;% 
  unnest() %&gt;% 
  mutate(.pred_class = ifelse(.pred_Yes &gt; .pred_No, &quot;Yes&quot;,
                              &quot;No&quot;),
         .pred_class = factor(.pred_class, levels = c(&quot;Yes&quot;,
                                                      &quot;No&quot;))) %&gt;% 
  filter(Id != &quot;0&quot;) %&gt;% 
  filter(!is.na(.pred_class)) %&gt;% 
  group_by(id, id2, model) %&gt;% 
  mutate(classes = n_distinct(.pred_class)) %&gt;% 
  ungroup() %&gt;% 
  filter(classes == 2) %&gt;% 
  group_by(model) %&gt;% 
  mutate(folds = max(row_number())) %&gt;% 
  ungroup() %&gt;% 
  group_by(id, id2, model) 

# calculate performance metrics
classifier.summary &lt;- tune_summary(folds.pred)

# rank classifiers
class.rank &lt;- tune_rank(classifier.summary, .met = metric$.metric)

# prep for a/b tests
class.test &lt;- tune_test(classifier.summary, .met = metric$.metric)

# bayes ttest
class.ab &lt;- ab_test(class.rank, class.test, .met = metric$.metric) %&gt;% 
  left_join(model.labels %&gt;% 
              select(better = model,
                     b = long)) %&gt;% 
  left_join(model.labels %&gt;% 
              select(than = model,
                     w = long))

# selected classifier
classifier &lt;- class.rank[class.rank$rank == 1, ]$model

# plot titles
class.title &lt;- paste0(model.labels[model.labels$model ==
                                     classifier,]$long,
                      &quot; averaged &quot;,
                      ifelse(metric$.metric == &quot;mn_log_loss&quot;,
                             round(class.rank[class.rank$rank == 1,
                                       ]$.estimate, 3),
                             round(class.rank[class.rank$rank == 1,
                                       ]$.estimate, 3) *100),
                      &quot;% &quot;,
                      str_to_lower(metric$metric))
                      
class.subtitle &lt;- prose_title(class.ab, .type = &quot;classifier&quot;)

# model specs
classifier.spec &lt;- models[[classifier]]

plot_folds(classifier.summary, class.title, class.subtitle)</code></pre>
<p><img src="demo_files/figure-html/classifierSelect-1.png" width="576" /></p>
</div>
<div id="feature-selection" class="section level3">
<h3>Feature Selection</h3>
<p>Xtreme Gradient Boosting model was fit to 6 different subsets of features.</p>
<pre class="r"><code># this chunk takes a while
# split folds
# prep recipe on analysis set for each fold
folds.internal &lt;- fold_recipe(folds.internal.fs, .rec = rec.all) %&gt;% 
  select(-form, -splits)

# bake analysis and assessment sets
# run feature selection methods
# make long
folds.internal &lt;- folds.internal %&gt;% 
  mutate(All = map(analysis, colnames),
         All = map(All, setdiff, c(&quot;Id&quot;, &quot;label&quot;)),
         Importance = map2(analysis, map(prep, formula), var_imp),
         Relevance = map2(analysis, map(prep, formula), var_rel),
         Significance = map2(analysis, All, var_sig),
         Exclusive = pmap(list(Importance, Relevance, Significance),
                          triple_inter),
         Inclusive = pmap(list(Importance, Relevance, Significance),
                          triple_u)) %&gt;%  
  select(-prep) %&gt;% 
  gather(fs, forms, -c(id, id2, analysis, assessment))

# fit models and predict assessment sets
folds.internal &lt;- folds.internal %&gt;% 
    mutate(vrs = map(forms, length)) %&gt;% 
    filter(vrs &gt; 0) %&gt;% 
    mutate(forms = map(forms, as_formula),
           fits = map2(analysis, forms, prs_form,
                            prs = classifier.spec),
           preds = map2(fits,  assessment, predict,
                           type = &quot;prob&quot;))

# average vars selected by each fs
folds.vrs &lt;- folds.internal %&gt;% 
  select(fs, vrs) %&gt;% 
  unnest() %&gt;% 
  group_by(fs) %&gt;% 
  summarize(vrs = round(mean(vrs)))

# to unnest list columns, df list-elements need equal rows
# get rows to pad by list element
pads &lt;- map_dbl(folds.internal$preds, nrow)
pads &lt;- max(pads) - pads

# pad df
pad.df &lt;- as_tibble(list(&quot;Id&quot; = &quot;0&quot;))

# list of df to pad truth column
pads.id &lt;- map(pads, df_pad, df = pad.df)

# list of df to pad prediction columns
pad.df &lt;- as_tibble(list(&quot;.pred_Yes&quot; = 0))
pads.pd &lt;- map(pads, df_pad, df = pad.df)

# evenup rows
# gather and unnest
# add naive class
# drop padding
folds.pred &lt;- folds.internal %&gt;% 
  mutate(truth = map(assessment, ~select(., Id, label))) %&gt;% 
  select(id, id2, model = fs, truth, preds) %&gt;% 
  mutate(truth = map2(truth, pads.id, bind_rows),
         preds = map2(preds, pads.pd, bind_rows)) %&gt;% 
  unnest() %&gt;% 
  mutate(.pred_class = ifelse(.pred_Yes &gt; .pred_No, &quot;Yes&quot;,
                              &quot;No&quot;),
         .pred_class = factor(.pred_class, levels = c(&quot;Yes&quot;,
                                                      &quot;No&quot;))) %&gt;% 
  filter(Id != &quot;0&quot;) %&gt;% 
  filter(!is.na(.pred_class)) %&gt;% 
  group_by(id, id2, model) %&gt;% 
  mutate(classes = n_distinct(.pred_class)) %&gt;% 
  ungroup() %&gt;% 
  filter(classes == 2) %&gt;% 
  group_by(model) %&gt;% 
  mutate(folds = max(row_number())) %&gt;% 
  ungroup() %&gt;% 
  filter(folds &gt; (max(folds) * 0.7)) %&gt;% 
  group_by(id, id2, model) 

# calculate performance metrics
fs.summary &lt;- tune_summary(folds.pred)

# rank
fs.rank &lt;- tune_rank(fs.summary, .met = metric$.metric)

# prep for a/b test
fs.test &lt;- tune_test(fs.summary, .met = metric$.metric)

# ttest
fs.ab &lt;- ab_test(fs.rank, fs.test, .met = metric$.metric)

# if second feature selection method performs similar to first
# and has less variables 
# select second fs
fs.same &lt;- fs.ab %&gt;% 
  filter(pair == 1) %&gt;% 
  left_join(folds.vrs %&gt;% 
              select(than = fs,
                     than.vrs = vrs)) %&gt;% 
  left_join(folds.vrs %&gt;% 
              select(better = fs,
                     better.vrs = vrs)) %&gt;% 
  mutate(smaller = than.vrs &lt;better.vrs) %&gt;% 
  arrange(desc(prob)) %&gt;% 
  mutate(.metric = metric$.metric) %&gt;% 
  filter(str_detect(key, &quot;=&quot;) &amp;
           smaller == T ) 
  
# plot subtitle
fs.subtitle &lt;- prose_title(fs.ab, 
                        .same = fs.same, 
                        .type = &quot;feature selector&quot;)

# selected fs
fs &lt;- ifelse(nrow(fs.same) == 1, 
             fs.same$than,
             fs.rank[fs.rank$rank == 1, ]$model)

# plot title
fs.title &lt;- paste0(&quot;Average &quot;,
                      str_to_lower(metric$metric),
                      &quot; of &quot;,
                      ifelse(metric$.metric == &quot;mn_log_loss&quot;,
                             round(fs.rank[fs.rank$rank == 1,
                                       ]$.estimate, 3),
                             round(fs.rank[fs.rank$rank == 1,
                                       ]$.estimate, 3) *100),
                   &quot;%&quot;)

plot_folds(fs.summary, fs.title, fs.subtitle)</code></pre>
<p><img src="demo_files/figure-html/featureselection-1.png" width="576" /></p>
</div>
<div id="hyperparameters" class="section level3">
<h3>Hyperparameters</h3>
<pre class="r"><code># prep for hyperparameter tests 
folds.internal &lt;- fold_recipe(folds.internal.tn, .rec = rec.all) %&gt;% 
  rename(Formula = form) %&gt;% 
  mutate(vrs = map(analysis, ~select(., -Id, -label)),
         vrs = map(vrs, colnames))

# apply selected fs
folds.internal &lt;- if(fs == &quot;Importance&quot;){
       folds.internal %&gt;% 
         select(-splits) %&gt;% 
         mutate(vrs = map2(analysis, Formula, var_imp),
                form = map(vrs, as_formula)) %&gt;% 
         select(-Formula)
      }else{
        if(fs == &quot;Inclusive&quot;){
          folds.internal %&gt;% 
            select(-splits) %&gt;% 
            mutate(Importance =  map2(analysis, Formula, var_imp),
                   Relevance = map2(analysis, Formula, var_rel),
                   Significance = map2(analysis, vrs, var_sig),
                   vrs = pmap(list(Importance,
                                   Relevance, 
                                   Significance), 
                              triple_u),
                   form = map(vrs, as_formula)) %&gt;% 
            select(-Formula)
          }else{
            if(fs == &quot;Exclusive&quot;){
              folds.internal %&gt;% 
                select(-splits) %&gt;% 
                mutate(Importance =  map2(analysis, Formula, var_imp),
                       Relevance = map2(analysis, Formula, var_rel),
                       Significance = map2(analysis, vrs, var_sig),
                       vrs = pmap(list(Importance,
                                       Relevance, 
                                       Significance),
                                  triple_inter),
                       form = map(vrs, as_formula))
              }else{
                if(fs == &quot;Relevance&quot;){
                  folds.internal %&gt;% 
                    select(-splits) %&gt;% 
                    mutate(vrs = map2(analysis, Formula, var_rel),
                           form = map(vrs, as_formula)) %&gt;% 
                    select(-Formula)
                  }else{
                    if(fs == &quot;Significance&quot;){
                      folds.internal %&gt;% 
                        select(-splits, -Formula) %&gt;% 
                        mutate(vrs = map2(analysis, vrs, var_sig),
                              form = map(vrs, as_formula))
                      }else{
                        folds.internal %&gt;% 
                          mutate(form = Formula) %&gt;% 
                          select(-splits, -Formula)
                      }
    }
    }
    }
}

# number of predictors
tn.cls &lt;- map_dbl(folds.internal$vrs, length)

# tuning parameter grid by model
tune.grid &lt;- list(
  &quot;RF&quot; = grid_random(mtry %&gt;% 
                     range_set(c(floor(sqrt(min(tn.cls))),
                                            min(tn.cls))),
                              min_n %&gt;% 
                               range_set(c(1,4)),
                             size = 9),
  &quot;SVM&quot; = grid_random(cost, 
                      rbf_sigma,
                      size = 9),
   &quot;XGB&quot; = grid_random(tree_depth, 
                       learn_rate %&gt;% 
                       range_set(c(0.1, 0.5)),
                       size = 9)
                  )

# clean model specs to merge with tuning parameters
raw.mdls &lt;- list(
  &quot;RF&quot; = rand_forest(mode = &quot;classification&quot;,
                     mtry = varying(),
                     min_n = varying(),
                     trees = 2000) %&gt;% 
                   set_engine(&quot;ranger&quot;,
                              importance = &quot;impurity&quot;,
                              probability = TRUE) %&gt;% 
                   merge(tune.grid[[&quot;RF&quot;]]),
  &quot;SVM&quot; = svm_rbf(mode = &quot;classification&quot;,
                           rbf_sigma = varying(),
                           cost = varying()) %&gt;% 
                   set_engine(&quot;kernlab&quot;,
                              kernel = &quot;rbfdot&quot;,
                              prob.model = T) %&gt;% 
                   merge(tune.grid[[&quot;SVM&quot;]]),
  &quot;XGB&quot; = boost_tree(mode = &quot;classification&quot;,
                              mtry = .preds(),
                              learn_rate = varying(),
                              tree_depth = varying()) %&gt;% 
                   set_engine(&quot;xgboost&quot;,
                              prob.model = T) %&gt;% 
                   merge(tune.grid[[&quot;XGB&quot;]])
  )

# selected model with tuning grid
# add on original model spec
prs.mdls &lt;- c(raw.mdls[[classifier]],
              list(models[[classifier]]))

# fit to resamples, looping over tuning grid
tunes &lt;- vector(&quot;list&quot;, length(prs.mdls))
for(i in(seq_along(prs.mdls))){
  tunes[[i]] &lt;- folds.internal %&gt;% 
    mutate(fit = map2(analysis, form, prs_form,
                      prs = prs.mdls[[i]]),
           fit = map2(fit, assessment, predict, type = &quot;prob&quot;),
           truth = map(assessment, ~select(., Id, label))) %&gt;% 
    select(starts_with(&quot;id&quot;), truth, fit)
  tunes
}

# bind rows of output
tunes &lt;- bind_rows(tunes, .id = &quot;model&quot;)

# to unnest list columns, df list-elements need equal rows
# get rows to pad by list element
pads &lt;- map_dbl(tunes$truth, nrow)
pads &lt;- max(pads) - pads

# pad df
pad.df &lt;- as_tibble(list(&quot;Id&quot; = &quot;0&quot;))

# list of df to pad truth column
pads.id &lt;- map(pads, df_pad, df = pad.df)

# list of df to pad prediction columns
pad.df &lt;- as_tibble(list(&quot;.pred_Yes&quot; = 0))
pads.pd &lt;- map(pads, df_pad, df = pad.df)

# apply hard class
tunes &lt;- tunes %&gt;% 
      mutate(truth = map2(truth, pads.id, bind_rows),
             fit = map2(fit, pads.pd, bind_rows)) %&gt;% 
      unnest() %&gt;% 
      mutate(.pred_class = ifelse(.pred_Yes &gt; .pred_No, &quot;Yes&quot;, &quot;No&quot;),
             .pred_class = factor(.pred_class,
                              levels = c(&quot;Yes&quot;, &quot;No&quot;))) %&gt;% 
      filter(Id != &quot;0&quot;)

# summary metrics
tunes.summary &lt;- tunes %&gt;% 
      group_by(model, id) %&gt;% 
      mutate(classes = n_distinct(.pred_class)) %&gt;% 
      ungroup() %&gt;% 
      filter(classes &gt; 1) %&gt;% 
      group_by(id, model) %&gt;% 
      conf_mat(truth = label, .pred_class) %&gt;% 
      mutate(summary = map(conf_mat, summary)) %&gt;% 
      select(starts_with(&quot;id&quot;), model, summary) %&gt;% 
      unnest()%&gt;% 
      bind_rows(tunes %&gt;% 
                  group_by(model, id) %&gt;% 
                  mn_log_loss(truth = label, .pred_Yes)) %&gt;% 
       mutate(comp = metric_benchmark(.metric, .estimate))

# rank tunes
tunes.rank &lt;- tune_rank(tunes.summary, .met = metric$.metric)

# prep for a/b test
tunes.test &lt;- tunes.summary %&gt;% 
      select(starts_with(&quot;id&quot;), model, .metric, .estimate) %&gt;% 
      filter(model %in% tunes.rank$model) %&gt;% 
      filter(.metric == metric$.metric) %&gt;% 
      spread(model, .estimate, fill = 0) %&gt;% 
      arrange(id)

# ttest
tunes.ab &lt;- ab_test(tunes.rank, tunes.test, .met = metric$.metric)

# selected tune
hyperparameters &lt;-as.numeric(tunes.rank[tunes.rank$rank ==
                                         1,]$model)[1]

tune.prose &lt;- paste0(length(prs.mdls),
        &quot; candidate hyperparameter combinations were&quot;,
        &quot; considered.\n&quot;,
        &quot;The probability the selected hyperparameters outperform&quot;, 
        &quot; the other options considered: &quot;,
        round(tunes.ab[1,]$combined,2)*100,
        &quot;%&quot;)</code></pre>
<p>10 candidate hyperparameter combinations were considered. The probability the selected hyperparameters outperform the other options considered: 91%.</p>
</div>
</div>
</div>
<div id="model" class="section level1 tabset tabset-pills">
<h1>Model</h1>
<pre class="r"><code># prep entire training set
prep.resample &lt;- prep(rec.all, training, strings_as_factors = F) 

# bake training set
train &lt;- bake(prep.resample, training)

# bake test set
test &lt;- bake(prep.resample, testing(initial.split))

# bake scoring set
score &lt;- bake(prep.resample, filter(objs, is.na(label)))

# get all predictors
train.all &lt;-  summary(prep.resample) %&gt;% 
  filter(role == &quot;predictor&quot;) %&gt;% 
  select(variable) %&gt;% 
  unlist()

# apply fs
train.vrs &lt;- if(fs == &quot;Importance&quot;){
  out &lt;- var_imp(train, formula(prep.resample))
}else{
  if(fs == &quot;Relevance&quot;){
    out &lt;- var_rel(train, formula(prep.resample))
  }else{
    if(fs == &quot;Significance&quot;){
      out &lt;- var_sig(train, train.all)
      }else{
        if(fs == &quot;Exclusive&quot;){
            imp &lt;- var_imp(train, formula(prep.resample))
            br &lt;- var_rel(train, formula(prep.resample))
            sg &lt;- var_sig(train, train.all)
            out &lt;- intersect(intersect(imp, br), intersect(br, sg))
        }else{
          if(fs == &quot;Inclusive&quot;){
            imp &lt;- var_imp(train, formula(prep.resample))
            br &lt;- var_rel(train, formula(prep.resample))
            sg &lt;- var_sig(train, train.all)
            out &lt;- union(imp, union(br, sg))
          }else{
        train.all
        }
        }
      }
  }
}

# create formula
train.form &lt;- as.formula(paste(&quot;label&quot;, &quot;~&quot;, 
                               paste(train.vrs, collapse = &quot;+&quot;)))

# fit model
fits &lt;- q_f(prs.mdls[[hyperparameters]],
            train.form,
            train)[[&quot;result&quot;]]

# generate test predictions
test.preds &lt;- test %&gt;% 
  nest(-c(Id, label)) %&gt;% 
  mutate(probs = map(data, df_pred, .fit = fits,
                     .type = &quot;prob&quot;)) %&gt;% 
  select(Id, label, probs) %&gt;% 
  unnest() %&gt;% 
  mutate(.pred_class = ifelse(.pred_Yes &gt; .pred_No, &quot;Yes&quot;, &quot;No&quot;),
         .pred_class = factor(.pred_class, 
                              levels = c(&quot;Yes&quot;, &quot;No&quot;))) %&gt;% 
  mutate(Correct = ifelse(label == .pred_class, &quot;Yes&quot;, &quot;No&quot;))

# generate new predictions
preds &lt;- score %&gt;% 
  nest(-c(Id, label)) %&gt;%
  mutate(probs = map(data, df_pred, .fit = fits, .type = &quot;prob&quot;),
         classes = map(data, df_pred,
                       .fit = fits, .type = &quot;class&quot;)) %&gt;% 
  select(Id, label, probs) %&gt;% 
  unnest() %&gt;% 
  mutate(.pred_class = ifelse(.pred_Yes &gt; .pred_No, &quot;Yes&quot;, &quot;No&quot;),
         .pred_class = factor(.pred_class, 
                              levels = c(&quot;Yes&quot;, &quot;No&quot;))) %&gt;% 
  mutate(Decile = floor(10 * (.pred_Yes / max(.pred_Yes))),
         Decile = ifelse(Decile == 10, 9, Decile))

# summary metrics
test.metrics &lt;- test.preds %&gt;% 
  conf_mat(truth = label, .pred_class) %&gt;% 
  summary() %&gt;% 
  bind_rows(test.preds %&gt;% 
              gain_capture(truth = label, .pred_Yes)) %&gt;%
  bind_rows(test.preds %&gt;% 
              pr_auc(truth = label, .pred_Yes)) %&gt;%
  bind_rows(test.preds %&gt;% 
              roc_auc(truth = label, .pred_Yes)) %&gt;%
  bind_rows(test.preds %&gt;% 
              mn_log_loss(truth = label, .pred_Yes)) %&gt;% 
  filter(!.metric %in% c(&quot;sens&quot;, &quot;spec&quot;, &quot;recall&quot;, &quot;precision&quot;)) %&gt;% 
  mutate(comp = metric_benchmark(.metric, .estimate),
         .estimate = round(.estimate, 3)) %&gt;% 
  inner_join(metric.table) %&gt;% 
  mutate(good = case_when(comp &lt; 0.95 ~ &quot;No&quot;,
                          comp &lt; 1.1 ~ &quot;Yes&quot;,
                          comp &gt;= 1.1 ~ &quot;Great&quot;))

vrs.prose &lt;- ifelse(fs != &quot;All&quot;,
                    paste(length(train.vrs), 
                          &quot;of&quot;,
                          length(train.all),
                          &quot;predictors selected&quot;,
                          &quot;by the&quot;,
                          fs,
                          &quot;filter&quot;),
                    paste(length(train.vrs),
                          &quot;predictors&quot;))</code></pre>
<p><strong>Final Model:</strong> Xtreme Gradient Boosting using 28 of 45 predictors selected by the Relevance filter.</p>
<div id="accuracy" class="section level2">
<h2>Accuracy</h2>
<pre class="r"><code># plot accuracy by year
acc.yr &lt;- test.preds %&gt;% 
  left_join(objs %&gt;% 
              select(Id, CreatedDate) %&gt;% 
              mutate(YearCreated = lubridate::year(CreatedDate))) %&gt;% 
  filter(YearCreated &gt;= lubridate::year(lubridate::as_date(age.limit))) %&gt;% 
  mutate(YearCreated = factor(YearCreated)) %&gt;% 
  group_by(YearCreated, Correct) %&gt;% 
  summarize(n = n()) %&gt;% 
  group_by(YearCreated) %&gt;% 
  mutate(total = sum(n)) %&gt;% 
  ungroup() %&gt;% 
  mutate(perc = n / total,
         rate = ifelse(Correct == &quot;No&quot; &amp; total &gt; 1, &quot;&quot;,
                       paste0(round(n / total, 2)*100, &quot;%&quot;)),
         clr = ifelse(total &lt;= 3, &quot;ble&quot;, &quot;whte&quot;),
         nudge = ifelse(total &lt;= 3, 1.5, -1.5)) 

acc.yr %&gt;%
  ggplot(aes(x = YearCreated, y = n))+
  geom_col(aes(fill = fct_rev(Correct)),
           color = &quot;#FFFFFF&quot;)+
  geom_text(aes(x = YearCreated,
                y = total + nudge, 
                color = clr,
                label = rate),
            size = 5,
            fontface = &quot;bold&quot;,
            show.legend = F)+
  scale_fill_manual(values = c(&quot;Yes&quot; = &quot;#377EB8&quot;,
                                &quot;No&quot; = &quot;#E41A1C&quot;))+
  scale_color_manual(values = c(&quot;whte&quot; = &quot;#FFFFFF&quot;,
                                &quot;ble&quot; = &quot;#132B43&quot;))+
  guides(fill = guide_legend(title = &quot;Correct&quot;))+
  labs(title = paste0(&quot;Overall Test Accuracy: &quot;,
                         round(test.metrics[test.metrics$.metric ==
                                              &quot;accuracy&quot;,
                                            ]$.estimate, 2)*100,
                         &quot;%&quot;),
       subtitle = 
         paste0(&quot;Accuracy for recent test records: &quot;,
                round(mean(filter(acc.yr, Correct == &quot;Yes&quot;)$perc), 2) * 100,
                &quot;%&quot;),
       x = &quot;Year Lead Created&quot;,
       y = &quot;Predictions&quot;)+
  theme(panel.background =
          element_rect(fill = &quot;white&quot;,
                       colour = &quot;grey50&quot;))</code></pre>
<p><img src="demo_files/figure-html/testplots-1.png" width="576" /></p>
<div id="confusion-matrix" class="section level3">
<h3>Confusion Matrix</h3>
<pre class="r"><code># plot confusion matrix
conf_mat(test.preds, label, .pred_class)[[1]] %&gt;% 
  as_tibble() %&gt;% 
  ggplot(aes(Prediction, Truth))+
  geom_tile(aes(alpha = n,
                fill = n),
            show.legend = F,
            color = &quot;grey50&quot;)+
  geom_text(aes(label = n,
                color = n),
            size = 8,
            show.legend = F,
            fontface = &quot;bold&quot;)+
  labs(title = paste(&quot;Confusion Matrix:&quot;,
                     nrow(test.preds[test.preds$Correct == &quot;Yes&quot;,]),
                     &quot;of&quot;,
                     nrow(test.preds),
                     &quot;test records correctly classified&quot;))+
  scale_y_discrete(limits = c(&quot;No&quot;, &quot;Yes&quot;))+
  scale_x_discrete(limits = c(&quot;Yes&quot;, &quot;No&quot;))+
  scale_color_continuous(low = &quot;#56B1F7&quot;, high = &quot;#132B43&quot;)+
  scale_fill_continuous(low = &quot;#132B43&quot;, high = &quot;#56B1F7&quot;)+
  theme(panel.background =
          element_rect(fill = &quot;white&quot;,
                       colour = &quot;grey50&quot;),
        axis.text = element_text(size = rel(1.2)))</code></pre>
<p><img src="demo_files/figure-html/confmat-1.png" width="576" /></p>
</div>
</div>
<div id="performance-curves" class="section level2 tabset tabset-pills">
<h2>Performance Curves</h2>
<div id="roc-curve" class="section level3">
<h3>ROC Curve</h3>
<pre class="r"><code># plot performance curves
test.preds %&gt;% 
  roc_curve(truth = label, .pred_Yes) %&gt;% 
  autoplot()+
  geom_text(data = test.metrics %&gt;% 
              filter(.metric == &quot;j_index&quot;),
            aes(label = paste0(metric, &quot;: &quot;, .estimate),
                color = good,
                x = 0.65,
                y = 0.2),
            size = 6,
            fontface = &quot;bold&quot;,
            show.legend = F)+
  scale_color_manual(values = c(&quot;Yes&quot; = &quot;#377EB8&quot;,
                                &quot;No&quot; = &quot;#E41A1C&quot;,
                                &quot;Great&quot; =  &quot;#4DAF4A&quot;))+
  labs(title = paste0(&quot;Area under curve&quot;,
                        &quot;: &quot;,
                        test.metrics[test.metrics$.metric ==
                                       &quot;roc_auc&quot;,]$.estimate),
       x = &quot;1 - Sensitivity&quot;,
       y = &quot;Specificity&quot;)+
  theme(panel.background =
          element_rect(fill = &quot;white&quot;,
                       colour = &quot;grey50&quot;),
        axis.text = element_text(size = rel(1)),
        panel.grid = element_blank(),
        plot.subtitle = element_text(size = rel(1.2)))</code></pre>
<p><img src="demo_files/figure-html/roccurve-1.png" width="576" /></p>
</div>
<div id="precision-recall-curve" class="section level3">
<h3>Precision Recall Curve</h3>
<pre class="r"><code>test.preds %&gt;% 
  pr_curve(truth = label, .pred_Yes) %&gt;% 
  autoplot()+
  geom_text(data = test.metrics %&gt;% 
              filter(.metric == &quot;f_meas&quot;),
            aes(label = paste0(metric, &quot;: &quot;, .estimate),
                color = good,
                x = 0.3,
                y = 0.2),
            size = 6,
            fontface = &quot;bold&quot;,
            show.legend = F)+
  scale_color_manual(values = c(&quot;Yes&quot; = &quot;#377EB8&quot;,
                                &quot;No&quot; = &quot;#E41A1C&quot;,
                                &quot;Great&quot; =  &quot;#4DAF4A&quot;))+
  labs(title = paste0(&quot;Area under curve&quot;,
                        &quot;: &quot;,
                        test.metrics[test.metrics$.metric ==
                                       &quot;pr_auc&quot;,]$.estimate),
       x = &quot;Recall&quot;,
       y = &quot;Precision&quot;)+
  coord_fixed(ratio = 1, ylim = c(0,1))+
  theme(panel.background =
          element_rect(fill = &quot;white&quot;,
                       colour = &quot;grey50&quot;),
        axis.text = element_text(size = rel(1)),
        panel.grid = element_blank(),
        plot.subtitle = element_text(size = rel(1.2)))</code></pre>
<p><img src="demo_files/figure-html/prcrv-1.png" width="576" /></p>
</div>
<div id="gain-curve" class="section level3">
<h3>Gain Curve</h3>
<pre class="r"><code>test.preds %&gt;% 
  gain_curve(truth = label, .pred_Yes) %&gt;% 
  autoplot()+
  geom_text(data = test.metrics %&gt;% 
              filter(.metric == &quot;mcc&quot;),
            aes(label = paste0(metric, &quot;: &quot;, .estimate),
                color = good,
                x = 70,
                y = 20),
            size = 6,
            fontface = &quot;bold&quot;,
            show.legend = F)+
  scale_color_manual(values = c(&quot;Yes&quot; = &quot;#377EB8&quot;,
                                &quot;No&quot; = &quot;#E41A1C&quot;,
                                &quot;Great&quot; =  &quot;#4DAF4A&quot;))+
  labs(title = paste0(&quot;Gain Capture&quot;,
                           &quot;: &quot;,
                        test.metrics[test.metrics$.metric ==
                                       &quot;gain_capture&quot;,]$.estimate))+
  theme(panel.background =
          element_rect(fill = &quot;white&quot;,
                       colour = &quot;grey50&quot;),
        axis.text = element_text(size = rel(1)),
        panel.grid = element_blank(),
        plot.subtitle = element_text(size = rel(1.2)))</code></pre>
<p><img src="demo_files/figure-html/gaincurve-1.png" width="576" /></p>
</div>
<div id="lift" class="section level3">
<h3>Lift</h3>
<pre class="r"><code>test.preds %&gt;% 
  lift_curve(truth = label, .pred_Yes) %&gt;% 
  autoplot()+
  geom_text(data = test.metrics %&gt;% 
              filter(.metric == &quot;bal_accuracy&quot;),
            aes(label = paste0(metric, &quot;: &quot;, .estimate),
                color = good,
                x = 25,
                y = 1.35),
            size = 6,
            fontface = &quot;bold&quot;,
            show.legend = F)+
  scale_color_manual(values = c(&quot;Yes&quot; = &quot;#377EB8&quot;,
                                &quot;No&quot; = &quot;#E41A1C&quot;,
                                &quot;Great&quot; =  &quot;#4DAF4A&quot;))+
  labs(title = paste0(&quot;Mean Log Loss&quot;,
                           &quot;: &quot;,
                        test.metrics[test.metrics$.metric ==
                                       &quot;mn_log_loss&quot;,]$.estimate))+ 
  theme(panel.background =
          element_rect(fill = &quot;white&quot;,
                       colour = &quot;grey50&quot;),
        axis.text = element_text(size = rel(1)),
        panel.grid = element_blank(),
        plot.subtitle = element_text(size = rel(1.2)))</code></pre>
<p><img src="demo_files/figure-html/lift-1.png" width="576" /></p>
</div>
</div>
<div id="training-improvement" class="section level2">
<h2>Training Improvement</h2>
<pre class="r"><code># compute performance during parameter selection
train.summary &lt;- tunes.summary %&gt;% 
  rename(id2 = id) %&gt;% 
  mutate(id = &quot;Repeat01&quot;,
         set = &quot;Hyperparameters&quot;) %&gt;% 
  bind_rows(fs.summary %&gt;% 
              mutate(set = &quot;Features&quot;)) %&gt;% 
  bind_rows(classifier.summary %&gt;% 
              mutate(set = &quot;Classifiers&quot;)) %&gt;% 
  left_join(test.metrics %&gt;% 
              select(.metric, test = .estimate)) %&gt;% 
  filter(!is.na(test)) %&gt;% 
  mutate(better = ifelse(.metric == &quot;mn_log_loss&quot;,
                         .estimate &lt; test,
                         .estimate &gt; test),
         selected = model %in% c(classifier,
                                 fs,
                                 hyperparameters),
         set = factor(set, levels = c(&quot;Classifiers&quot;,
                                      &quot;Features&quot;,
                                      &quot;Hyperparameters&quot;),
                      ordered = T))

# did performance improve
train.imp &lt;- train.summary %&gt;% 
  group_by(set, selected, .metric) %&gt;% 
  summarise(better = mean(better)) %&gt;% 
  ungroup() %&gt;% 
  spread(set, better) %&gt;% 
  mutate(improve = Classifiers &lt; Features |
           Features &lt; Hyperparameters,
         Test = !Classifiers %in% c(0,1) |
           !Features %in% c(0,1) |
           !Hyperparameters %in% c(0,1)) %&gt;% 
  filter(.metric == metric$.metric)

# plot  
train.summary %&gt;% 
  filter(.metric == metric$.metric) %&gt;% 
  group_by(set) %&gt;% 
  ggplot(aes(set,
             .estimate, 
             color = set))+
  geom_violin(scale = &quot;count&quot;, show.legend = F)+
  geom_hline(aes(yintercept = test.metrics[test.metrics$.metric ==
                                             metric$.metric,]$.estimate),
             color = &quot;blue&quot;)+
  scale_color_brewer(palette = &quot;Set1&quot;)+
  labs(title = ifelse(train.imp$Test == T,
                      paste(&quot;Test Performance inline&quot;, 
                            &quot;with Resampling Performance&quot;),
                      paste(&quot;Test Performance overly optimistic&quot;)),
       subtitle = ifelse(train.imp$improve == T,
                         paste(&quot;Model refinement validates&quot;, 
                               &quot;performance estimate&quot;),
                         paste(&quot;Model did not improve&quot;)),
       x = NULL)+
  theme(panel.background = element_rect(fill = &quot;white&quot;,
                                        colour = &quot;grey50&quot;),
        legend.key = element_rect(fill = &quot;white&quot;))</code></pre>
<p><img src="demo_files/figure-html/trainimp-1.png" width="576" /></p>
</div>
</div>
<div id="scores" class="section level1">
<h1>Scores</h1>
<pre class="r"><code># compute lift by decile
mdl.lift.decile &lt;- test.preds %&gt;%
  yardstick::lift_curve(truth = label, .pred_Yes) %&gt;% 
  filter(!is.na(.lift)) 

top.deciles &lt;- mdl.lift.decile %&gt;% 
  mutate(.lift = round(.lift, 3)) %&gt;% 
  filter(.lift == max(.lift))

decile.lift &lt;- 
  paste0(&quot;Tesing indicates &quot;,
          round(unique(top.deciles$.lift)),
         &quot;x lift for the top &quot;,
         round(max(top.deciles$.percent_tested)),
         &quot;%&quot;)</code></pre>
<p>Tesing indicates 4x lift for the top 20%.</p>
<div id="range-of-predicted-values" class="section level2">
<h2>Range of Predicted Values</h2>
<pre class="r"><code># bucket predictions by probability and plot
pred.dec &lt;- preds %&gt;% 
  mutate(Score = .pred_Yes / max(.pred_Yes),
         Decile = round(Score, 1)) %&gt;% 
  group_by(Decile) %&gt;% 
  summarize(Score = mean(Score),
            Records = n_distinct(Id)) %&gt;% 
  ungroup() %&gt;% 
  mutate(top = Decile &gt; (1 - (max(top.deciles$.percent_tested) / 100)))

pred.dec %&gt;% 
  ggplot(aes(Decile, Records, fill = Score))+
  geom_col(width = 0.1,
           color = &quot;white&quot;)+
  geom_text(data = filter(pred.dec, 
                          top == T),
            aes(label = Records,
                color = Score),
            fontface = &quot;bold&quot;,
            vjust = &quot;inward&quot;)+
  scale_x_continuous(breaks = seq(0, 1, 0.2),
                     labels = as.character(seq(0, 1, 0.2)))+
  guides(fill = &quot;none&quot;,
         color = &quot;none&quot;)+
  labs(title = paste(&quot;Range of class probabilities for&quot;,
                      nrow(preds),
                      object.label),
       subtitle = paste(sum(filter(pred.dec,
                                   top == T)$Records),
                        object.label,
                        &quot;w/ maximum lift&quot;),
       x = &quot;Class Probability&quot;,
       y = &quot;Predictions&quot;)+
  theme(panel.background =
          element_rect(fill = &quot;white&quot;,
                       colour = &quot;grey50&quot;))</code></pre>
<p><img src="demo_files/figure-html/predrange-1.png" width="576" /></p>
</div>
</div>
<div id="r" class="section level1 tabset">
<h1>R</h1>
<div id="session-info" class="section level2">
<h2>Session Info</h2>
<pre class="r"><code># session info
print(sessionInfo())</code></pre>
<pre><code>## R version 3.6.0 (2019-04-26)
## Platform: x86_64-pc-linux-gnu (64-bit)
## Running under: Ubuntu 18.04.2 LTS
## 
## Matrix products: default
## BLAS:   /usr/lib/x86_64-linux-gnu/openblas/libblas.so.3
## LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.2.20.so
## 
## locale:
##  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              
##  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    
##  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   
##  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 
##  [9] LC_ADDRESS=C               LC_TELEPHONE=C            
## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] caret_6.0-84      lattice_0.20-38   lubridate_1.7.4  
##  [4] yardstick_0.0.3   rsample_0.0.4     recipes_0.1.5    
##  [7] parsnip_0.0.2     infer_0.4.0.1     dials_0.0.2      
## [10] scales_1.0.0      broom_0.5.2       tidymodels_0.0.2 
## [13] forcats_0.4.0     stringr_1.4.0     dplyr_0.8.0.1    
## [16] purrr_0.3.2       readr_1.3.1       tidyr_0.8.3      
## [19] tibble_2.1.1      tidyverse_1.2.1   ggraph_1.0.2     
## [22] ggplot2_3.1.1     tidygraph_1.1.2   Boruta_6.0.0     
## [25] ranger_0.11.2     salesforcer_0.1.2
## 
## loaded via a namespace (and not attached):
##   [1] tidyselect_0.2.5    lme4_1.1-21         htmlwidgets_1.3    
##   [4] grid_3.6.0          pROC_1.14.0         munsell_0.5.0      
##   [7] codetools_0.2-16    xgboost_0.82.1      DT_0.5             
##  [10] miniUI_0.1.1.1      withr_2.1.2         colorspace_1.4-1   
##  [13] rgexf_0.15.3        knitr_1.22          rstudioapi_0.10    
##  [16] stats4_3.6.0        bayesplot_1.6.0     labeling_0.3       
##  [19] rstan_2.18.2        polyclip_1.10-0     farver_1.1.0       
##  [22] downloader_0.4      generics_0.0.2      ipred_0.9-9        
##  [25] xfun_0.6            R6_2.4.0            markdown_0.9       
##  [28] rstanarm_2.18.2     assertthat_0.2.1    promises_1.0.1     
##  [31] nnet_7.3-12         gtable_0.3.0        globals_0.12.4     
##  [34] processx_3.3.0      timeDate_3043.102   rlang_0.3.4        
##  [37] splines_3.6.0       lazyeval_0.2.2      ModelMetrics_1.2.2 
##  [40] acepack_1.4.1       brew_1.0-6          checkmate_1.9.1    
##  [43] inline_0.3.15       yaml_2.2.0          reshape2_1.4.3     
##  [46] modelr_0.1.4        tidytext_0.2.0      threejs_0.3.1      
##  [49] crosstalk_1.0.0     backports_1.1.4     httpuv_1.5.1       
##  [52] rsconnect_0.8.13    tokenizers_0.2.1    Hmisc_4.2-0        
##  [55] DiagrammeR_1.0.1    tools_3.6.0         lava_1.6.5         
##  [58] influenceR_0.1.0    ellipsis_0.1.0      RColorBrewer_1.1-2 
##  [61] ggridges_0.5.1      Rcpp_1.0.1          plyr_1.8.4         
##  [64] base64enc_0.1-3     visNetwork_2.0.6    ps_1.3.0           
##  [67] prettyunits_1.0.2   rpart_4.1-15        openssl_1.3        
##  [70] viridis_0.5.1       zoo_1.8-5           haven_2.1.0        
##  [73] ggrepel_0.8.0       cluster_2.0.8       magrittr_1.5       
##  [76] data.table_1.12.2   colourpicker_1.0    SnowballC_0.6.0    
##  [79] matrixStats_0.54.0  tidyposterior_0.0.2 hms_0.4.2          
##  [82] shinyjs_1.0         mime_0.6            evaluate_0.13      
##  [85] xtable_1.8-4        tidypredict_0.3.0   shinystan_2.5.0    
##  [88] XML_3.98-1.19       readxl_1.3.1        gridExtra_2.3      
##  [91] rstantools_1.5.1    compiler_3.6.0      crayon_1.3.4       
##  [94] minqa_1.2.4         StanHeaders_2.18.1  htmltools_0.3.6    
##  [97] later_0.8.0         Formula_1.2-3       tweenr_1.0.1       
## [100] MASS_7.3-51.4       boot_1.3-22         Matrix_1.2-17      
## [103] cli_1.1.0           parallel_3.6.0      gower_0.2.0        
## [106] igraph_1.2.4.1      pkgconfig_2.0.2     foreign_0.8-71     
## [109] foreach_1.4.4       xml2_1.2.0          dygraphs_1.1.1.6   
## [112] prodlim_2018.04.18  rvest_0.3.3         janeaustenr_0.1.5  
## [115] callr_3.2.0         digest_0.6.18       rmarkdown_1.12     
## [118] cellranger_1.1.0    htmlTable_1.13.1    Rook_1.1-1         
## [121] curl_3.3            kernlab_0.9-27      shiny_1.3.2        
## [124] gtools_3.8.1        nloptr_1.2.1        nlme_3.1-139       
## [127] jsonlite_1.6        viridisLite_0.3.0   askpass_1.1        
## [130] pillar_1.3.1        loo_2.1.0           httr_1.4.0         
## [133] pkgbuild_1.0.3      survival_2.44-1.1   glue_1.3.1         
## [136] xts_0.11-2          iterators_1.0.10    shinythemes_1.1.2  
## [139] glmnet_2.0-16       ggforce_0.2.2       class_7.3-15       
## [142] stringi_1.4.3       latticeExtra_0.6-28 e1071_1.7-1</code></pre>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
